{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting and Understanding Word Embedding/Vectors\n",
    "# Using the GloVe Embeddings\n",
    "\n",
    "\n",
    "Word embeddings (also known as word vectors) are a way to encode the meaning of words into a set of numbers.\n",
    "\n",
    "These embeddings are created by training a neural network model using many examples of the use of language.  These examples could be the whole of Wikipedia or a large collection of news articles. \n",
    "\n",
    "To start, we will explore a set of word embeddings that someone else took the time and computational power to create. One of the most commonly-used pre-trained word embeddings are the **GloVe embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Embeddings\n",
    "\n",
    "You can read about the GloVe embeddings here: https://nlp.stanford.edu/projects/glove/, and read the original paper describing how they work here: https://nlp.stanford.edu/pubs/glove.pdf.\n",
    "\n",
    "There are several variations of GloVe embeddings. They differ in the text used to train the embedding, and the *size* of the embeddings.\n",
    "\n",
    "We'll begin by loading a set of GloVe embeddings. The first time you run the code below, it will cause the download a large file (862MB) containing the embeddings.\n",
    "\n",
    "Note that as of 2025, there is now a new version of Glove Embeddings provided on that website, but for now the old ones will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and extract GloVe vectors\n",
    "def download_glove(glove_dir='./glove', glove_dim=50):\n",
    "    \"\"\"\n",
    "    Download GloVe vectors from Stanford NLP repository\n",
    "\n",
    "    Args:\n",
    "        glove_dir: Directory to save GloVe vectors\n",
    "        glove_dim: Dimension of GloVe vectors (50, 100, 200, or 300)\n",
    "\n",
    "    Returns:\n",
    "        Path to the downloaded GloVe file\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(glove_dir, exist_ok=True)\n",
    "\n",
    "    # GloVe URL based on dimension\n",
    "    glove_urls = {\n",
    "        50: 'http://nlp.stanford.edu/data/glove.6B.zip',\n",
    "        100: 'http://nlp.stanford.edu/data/glove.6B.zip',\n",
    "        200: 'http://nlp.stanford.edu/data/glove.6B.zip',\n",
    "        300: 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "    }\n",
    "\n",
    "    if glove_dim not in glove_urls:\n",
    "        raise ValueError(f\"Invalid dimension. Choose from {list(glove_urls.keys())}\")\n",
    "\n",
    "    url = glove_urls[glove_dim]\n",
    "    zip_path = os.path.join(glove_dir, 'glove.6B.zip')\n",
    "\n",
    "    # Download if not already present\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"Downloading GloVe vectors from {url}...\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "\n",
    "    # Extract the specific dimension file\n",
    "    glove_file = f'glove.6B.{glove_dim}d.txt'\n",
    "    glove_path = os.path.join(glove_dir, glove_file)\n",
    "\n",
    "    if not os.path.exists(glove_path):\n",
    "        print(f\"Extracting {glove_file}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extract(glove_file, glove_dir)\n",
    "\n",
    "    return glove_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load GloVe vectors into PyTorch tensors,\n",
    "# and provide mappings from words to indices and vice versa\n",
    "\n",
    "def load_glove_vectors(glove_path, vocab_size=None):\n",
    "    \"\"\"\n",
    "    Load GloVe vectors from file into PyTorch tensors\n",
    "\n",
    "    Args:\n",
    "        glove_path: Path to GloVe text file\n",
    "        vocab_size: Number of words to load (None for all)\n",
    "\n",
    "    Returns:\n",
    "        word2idx: Dictionary mapping words to indices\n",
    "        idx2word: List mapping indices to words\n",
    "        embeddings: PyTorch tensor of embeddings\n",
    "    \"\"\"\n",
    "    print(f\"Loading GloVe vectors from {glove_path}...\")\n",
    "\n",
    "    word2idx = {}\n",
    "    idx2word = []\n",
    "    vectors = []\n",
    "\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(tqdm(f)):\n",
    "            if vocab_size and i >= vocab_size:\n",
    "                break\n",
    "\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "\n",
    "            word2idx[word] = i\n",
    "            idx2word.append(word)\n",
    "            vectors.append(vector)\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    embeddings = torch.from_numpy(np.array(vectors))\n",
    "\n",
    "    print(f\"Loaded {len(word2idx)} words with dimension {embeddings.shape[1]}\")\n",
    "\n",
    "    return word2idx, idx2word, embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe vectors from ./glove\\glove.6B.50d.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:04, 94173.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 words with dimension 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Use above functions to Download and load GloVe vectors\n",
    "\n",
    "glove_path = download_glove(glove_dim=50) # embedding size = 50\n",
    "\n",
    "word2idx, idx2word, embeddings = load_glove_vectors(glove_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a useful function for accessing GloVe\n",
    "\n",
    "def glove(word):\n",
    "    \"\"\"Return the GloVe vector for a given word.\"\"\"\n",
    "    idx = word2idx.get(word)\n",
    "    if idx is not None:\n",
    "        return embeddings[idx]\n",
    "    else:\n",
    "        raise KeyError(f\"Word '{word}' not found in GloVe vocabulary.\") #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now that's done let's look at what the embedding of the word \"apple\" looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5204, -0.8314,  0.4996,  1.2893,  0.1151,  0.0575, -1.3753, -0.9731,\n",
      "         0.1835,  0.4767, -0.1511,  0.3553,  0.2591, -0.7786,  0.5218,  0.4769,\n",
      "        -1.4251,  0.8580,  0.5982, -1.0903,  0.3357, -0.6089,  0.4174,  0.2157,\n",
      "        -0.0742, -0.5822, -0.4502,  0.1725,  0.1645, -0.3841,  2.3283, -0.6668,\n",
      "        -0.5818,  0.7439,  0.0950, -0.4787, -0.8459,  0.3870,  0.2369, -1.5523,\n",
      "         0.6480, -0.1652, -1.4719, -0.1622,  0.7986,  0.9739,  0.4003, -0.2191,\n",
      "        -0.3094,  0.2658])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = glove('apple')\n",
    "print(e)\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it is a torch tensor with dimension `(50,)`. We don't know what the meaning of each number is, but we do know that there are properties of the embeddings that can be observed.  For example, `distances between embeddings` are meaningful:\n",
    "\n",
    "## Measuring Distance\n",
    "\n",
    "Let's consider one specific metric of distance between two embedding vectors called the **Euclidean distance**. The Euclidean distance of two vectors $x = [x_1, x_2, ... x_n]$ and\n",
    "$y = [y_1, y_2, ... y_n]$ is just the 2-norm of their difference $x - y$. We can compute\n",
    "the Euclidean distance between $x$ and $y$: $\\sqrt{\\sum_i (x_i - y_i)^2}$\n",
    "\n",
    "The PyTorch function `torch.norm` computes the 2-norm of a vector for us, so we \n",
    "can compute the Euclidean distance between two vectors like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8846)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = glove('cat')\n",
    "y = glove('dog')\n",
    "torch.norm(y - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7721)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = glove('apple')\n",
    "b = glove('banana')\n",
    "torch.norm(b - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3189)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(glove('good') - glove('bad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3390)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(glove('good') - glove('water'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8834)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(glove('good') - glove('perfect'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "An alternative and more commonly-sued measure of distance is the **Cosine Similarity**. The cosine similarity measures the *angle* between two vectors, and has the property that it only considers the *direction* of the vectors, not their the magnitudes. It is computed as follows for two vectors A and B:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\text{similarity} = \\cos(\\theta) = \n",
    "\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} \n",
    "= \\frac{\\sum_{i=1}^n A_i B_i}{\\sqrt{\\sum_{i=1}^n A_i^2} \\sqrt{\\sum_{i=1}^n B_i^2}},\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 1., 1.]).unsqueeze(0) # cosine similarity wants at least 2-D inputs\n",
    "y = torch.tensor([2., 2., 2.]).unsqueeze(0)\n",
    "\n",
    "torch.cosine_similarity(x, y) # should be one because x and y point in the same \"direction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.tensor([-1., -1., -1.]).unsqueeze(0)\n",
    "torch.cosine_similarity(x, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity is actually a *similarity* measure rather than a *distance* measure, and gives a result between -1 and 1. Thus, the larger the similarity, (closer to 1) the \"closer in meaning\" the word embeddings are to each other.  Note that even though the above result is -1, most of the actually results for word differences are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9218])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = glove('cat')\n",
    "y = glove('dog')\n",
    "torch.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5388])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = glove('apple')\n",
    "b = glove('orange')\n",
    "torch.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7965])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(glove('good').unsqueeze(0), \n",
    "                        glove('bad').unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8376])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(glove('good').unsqueeze(0), \n",
    "                        glove('perfect').unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: torch.cosine_similarity requires two dimensions to work, which is created with the unsqueeze option, illustrated in more detail below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50])\n",
      "torch.Size([1, 50])\n"
     ]
    }
   ],
   "source": [
    "x = glove('good')\n",
    "print(x.shape) # [50]\n",
    "y = x.unsqueeze(0) # [1, 50]\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity\n",
    "\n",
    "Now that we have notions of distance and similarity in our embedding space, we can talk about words that are \"close\" to each other in the embedding space. For now, let's use Euclidean distances to look at how close various words are to the word \"cat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pet \t 3.04\n",
      "dog \t 1.88\n",
      "bike \t 5.05\n",
      "kitten \t 3.51\n",
      "puppy \t 3.06\n",
      "kite \t 4.21\n",
      "computer \t 6.03\n",
      "neuron \t 6.23\n"
     ]
    }
   ],
   "source": [
    "word = 'cat'\n",
    "other = ['pet', 'dog', 'bike', 'kitten', 'puppy', 'kite', 'computer', 'neuron']\n",
    "for w in other:\n",
    "    dist = torch.norm(glove(word) - glove(w)) # euclidean distance\n",
    "    print(w, \"\\t%5.2f\" % float(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing with cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pet \t 0.78\n",
      "dog \t 0.92\n",
      "bike \t 0.44\n",
      "kitten \t 0.64\n",
      "puppy \t 0.76\n",
      "kite \t 0.49\n",
      "computer \t 0.35\n",
      "neuron \t 0.21\n"
     ]
    }
   ],
   "source": [
    "word = 'cat'\n",
    "other = ['pet', 'dog', 'bike', 'kitten', 'puppy', 'kite', 'computer', 'neuron']\n",
    "for w in other:\n",
    "    dist = torch.cosine_similarity(glove(word).unsqueeze(0),glove(w).unsqueeze(0)) # cosine distance\n",
    "    print(w, \"\\t%5.2f\" % float(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look through the entire **vocabulary** for words that are closest to a point in the embedding space -- for example, we can look for words that are closest to another word such as \"cat\".  This simple function, which sorts the entire vocabulary by the distance from a given word, is **surprisingly powerful**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print closest words to a given vector\n",
    "# This function computes the distances from the given vector to all words in the GloVe embeddings\n",
    "# and prints the top n closest words along with their distances.\n",
    "\n",
    "def print_closest_words(vec, n=5):\n",
    "    dists = torch.norm(embeddings - vec, dim=1)     # compute distances to all words  (embeddings was glove.vectors)\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance\n",
    "    for idx, difference in lst[1:n+1]:                         # take the top n\n",
    "        print(idx2word[idx], \"\\t%5.2f\" % difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog \t 1.88\n",
      "rabbit \t 2.46\n",
      "monkey \t 2.81\n",
      "cats \t 2.90\n",
      "rat \t 2.95\n",
      "beast \t 2.99\n",
      "monster \t 3.00\n",
      "pet \t 3.04\n",
      "snake \t 3.06\n",
      "puppy \t 3.06\n"
     ]
    }
   ],
   "source": [
    "# what are the 10 closest words to \"cat\" in GloVe?\n",
    "print_closest_words(glove(\"cat\"), n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat \t 1.88\n",
      "dogs \t 2.65\n",
      "puppy \t 3.15\n",
      "rabbit \t 3.18\n",
      "pet \t 3.23\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('dog'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doctor \t 3.13\n",
      "dentist \t 3.13\n",
      "nurses \t 3.27\n",
      "pediatrician \t 3.32\n",
      "counselor \t 3.40\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('nurse'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computers \t 2.44\n",
      "software \t 2.93\n",
      "technology \t 3.19\n",
      "electronic \t 3.51\n",
      "computing \t 3.60\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('computer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "margaret \t 2.01\n",
      "mary \t 2.27\n",
      "anne \t 2.30\n",
      "catherine \t 2.62\n",
      "katherine \t 2.72\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('elizabeth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siu \t 3.20\n",
      "chan \t 3.21\n",
      "jen \t 3.43\n",
      "wong \t 3.45\n",
      "fong \t 3.56\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('chow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "care \t 2.64\n",
      "medical \t 3.24\n",
      "welfare \t 3.62\n",
      "prevention \t 3.76\n",
      "education \t 3.76\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('health'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persistent \t 3.23\n",
      "experiencing \t 3.25\n",
      "discomfort \t 3.29\n",
      "nervousness \t 3.29\n",
      "anxieties \t 3.30\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('anxiety'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also look at which words are closest to the midpoints of two words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy \t 1.92\n",
      "feels \t 2.36\n",
      "sorry \t 2.50\n",
      "hardly \t 2.53\n",
      "imagine \t 2.57\n"
     ]
    }
   ],
   "source": [
    "print_closest_words((glove('happy') + glove('sad')) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surrounding \t 3.07\n",
      "nearby \t 3.11\n",
      "bridge \t 3.16\n",
      "along \t 3.16\n",
      "shore \t 3.16\n"
     ]
    }
   ],
   "source": [
    "print_closest_words((glove('lake') + glove('building')) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "farrell \t 2.80\n",
      "anderson \t 2.85\n",
      "jacobs \t 2.85\n",
      "boyle \t 2.86\n",
      "slater \t 2.87\n"
     ]
    }
   ],
   "source": [
    "print_closest_words((glove('bravo') + glove('michael')) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ten \t 1.57\n",
      "only \t 1.88\n",
      "three \t 2.03\n",
      "five \t 2.05\n",
      "four \t 2.11\n"
     ]
    }
   ],
   "source": [
    "print_closest_words((glove('one') + glove('ten')) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogies\n",
    "\n",
    "One surprising aspect of word embeddings is that the *directions* in the embedding space can be meaningful. For example, some analogy-like relationships like this tend to hold:\n",
    "\n",
    "$$ king - man + woman \\approx queen $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen \t 2.84\n",
      "prince \t 3.66\n",
      "elizabeth \t 3.72\n",
      "daughter \t 3.83\n",
      "widow \t 3.85\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('king') - glove('man') + glove('woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top result is a reasonable answer like \"queen\",  and the name of the queen of england.\n",
    "\n",
    "We can flip the analogy around and it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king \t 2.84\n",
      "prince \t 3.25\n",
      "crown \t 3.45\n",
      "knight \t 3.56\n",
      "coronation \t 3.62\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('queen') - glove('woman') + glove('man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, try a different but related analogies along a gender axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen \t 3.18\n",
      "king \t 3.91\n",
      "bride \t 4.29\n",
      "lady \t 4.30\n",
      "sister \t 4.42\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('king') - glove('prince') + glove('princess'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grandmother \t 2.32\n",
      "aunt \t 2.35\n",
      "granddaughter \t 2.36\n",
      "daughter \t 2.40\n",
      "uncle \t 2.60\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('uncle') - glove('man') + glove('woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncle \t 2.08\n",
      "father \t 2.09\n",
      "grandson \t 2.30\n",
      "nephew \t 2.35\n",
      "elder \t 2.43\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('grandmother') - glove('mother') + glove('father'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father \t 4.03\n",
      "son \t 4.41\n",
      "grandfather \t 4.52\n",
      "grandson \t 4.72\n",
      "daughter \t 4.79\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('old') - glove('young') + glove('father'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also move an embedding towards the direction of \"goodness\" or \"badness\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versatile \t 4.38\n",
      "creative \t 4.57\n",
      "entrepreneur \t 4.63\n",
      "enables \t 4.72\n",
      "intelligent \t 4.73\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('good') - glove('bad') + glove('programmer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hacker \t 3.84\n",
      "glitch \t 4.00\n",
      "originator \t 4.04\n",
      "hack \t 4.05\n",
      "serial \t 4.23\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('bad') - glove('good') + glove('programmer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias in Word Vectors\n",
    "\n",
    "While it may appear that machine learning models have an implicit air of \"fairness\" about them, because the models\n",
    "make decisions without human intervention. However, models can and do learn whatever bias is present in the training data - in this case the bias is present in the text that the vectors were trained on.\n",
    "\n",
    "Below are some examples that show that the structure of the GloVe vectors encodes the everyday biases present in the texts that they are trained on.\n",
    "\n",
    "We'll start with an example analogy:\n",
    "\n",
    "$$doctor - man + woman \\approx ??$$\n",
    "\n",
    "Using GloVe vectors to find the answer to the above analogy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nurse \t 3.14\n",
      "pregnant \t 3.78\n",
      "child \t 3.78\n",
      "woman \t 3.86\n",
      "mother \t 3.92\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('doctor') - glove('man') + glove('woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $$doctor - man + woman \\approx nurse$$ analogy is very concerning.\n",
    "Just to verify, the same result does not appear if we flip the gender terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man \t 3.93\n",
      "colleague \t 3.98\n",
      "himself \t 3.98\n",
      "brother \t 4.00\n",
      "another \t 4.03\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('doctor') - glove('woman') + glove('man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see similar types of gender bias with other professions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prodigy \t 3.67\n",
      "psychotherapist \t 3.81\n",
      "therapist \t 3.81\n",
      "introduces \t 3.91\n",
      "swedish-born \t 4.12\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('programmer') - glove('man') + glove('woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the first result, none of the other words are even related to\n",
    "programming! In contrast, if we flip the gender terms, we get very\n",
    "different results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup \t 4.00\n",
      "innovator \t 4.07\n",
      "programmers \t 4.17\n",
      "hacker \t 4.23\n",
      "genius \t 4.36\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('programmer') - glove('woman') + glove('man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results for \"engineer\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technician \t 3.69\n",
      "mechanic \t 3.92\n",
      "pioneer \t 4.15\n",
      "pioneering \t 4.19\n",
      "educator \t 4.23\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('engineer') - glove('man') + glove('woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builder \t 4.35\n",
      "mechanic \t 4.40\n",
      "engineers \t 4.48\n",
      "worked \t 4.53\n",
      "replacing \t 4.60\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove('engineer') - glove('woman') + glove('man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that these vectors are old, and newer vectors should be better.  Hopefully the new ones on the Glove website are improved!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
