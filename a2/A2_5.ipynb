{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b25d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Manual GloVe loader\n",
    "def load_glove(file_path):\n",
    "    word2idx = {}\n",
    "    vectors = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vec = np.array(values[1:], dtype=np.float32)\n",
    "            word2idx[word] = i\n",
    "            vectors.append(vec)\n",
    "    # Add OOV token at the end\n",
    "    word2idx[\"<OOV>\"] = len(vectors)\n",
    "    vectors.append(np.zeros_like(vectors[0]))\n",
    "    glove_vectors = torch.tensor(np.stack(vectors))\n",
    "    return word2idx, glove_vectors\n",
    "\n",
    "# Load GloVe embeddings\n",
    "word2idx, glove_vectors = load_glove(\"glove/glove.6B.100d.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee8bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(file_path=\"data/data.tsv\"):\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=40, stratify=df.label)\n",
    "    train, val = train_test_split(train, test_size=0.2, random_state=40, stratify=train.label)\n",
    "\n",
    "    # Save splits\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    train.to_csv(\"data/train.tsv\", sep=\"\\t\", index=False)\n",
    "    val.to_csv(\"data/validation.tsv\", sep=\"\\t\", index=False)\n",
    "    test.to_csv(\"data/test.tsv\", sep=\"\\t\", index=False)\n",
    "    overfit = df.sample(n=50)\n",
    "    overfit.to_csv(\"data/overfit.tsv\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6db4d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adcc7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split=\"train\"):\n",
    "        df = pd.read_csv(f\"data/{split}.tsv\", sep=\"\\t\")\n",
    "        self.texts = df.text.tolist()\n",
    "        self.labels = df.label.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "def my_collate_function(batch, device, word2idx=word2idx):\n",
    "    texts, labels = zip(*batch)\n",
    "    max_len = max(len(t.split()) for t in texts)\n",
    "    tokenized = []\n",
    "    for t in texts:\n",
    "        tokens = [word2idx.get(w, word2idx[\"<OOV>\"]) for w in t.split()]\n",
    "        tokens += [word2idx[\"<OOV>\"]] * (max_len - len(tokens))\n",
    "        tokenized.append(tokens)\n",
    "    return torch.tensor(tokenized, dtype=torch.long).to(device), torch.tensor(labels, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d0a8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, glove_vectors, k1, k2, n1, n2, freeze=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove_vectors, freeze=freeze)\n",
    "        self.k1 = (k1, glove_vectors.shape[1])\n",
    "        self.k2 = (k2, glove_vectors.shape[1])\n",
    "        self.n1 = n1\n",
    "        self.n2 = n2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, n1, self.k1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(n1)\n",
    "        self.pool1 = nn.AdaptiveMaxPool2d((1,1))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(1, n2, self.k2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(n2)\n",
    "        self.pool2 = nn.AdaptiveMaxPool2d((1,1))\n",
    "\n",
    "        self.out = nn.Linear(n1 + n2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e = self.embedding(x)  # (seq_len, batch, embed)\n",
    "        e = e.transpose(0,1).unsqueeze(1)  # (batch,1,seq_len,embed)\n",
    "        x1 = F.relu(self.bn1(self.conv1(e)))\n",
    "        x1 = self.pool1(x1)\n",
    "        x2 = F.relu(self.bn2(self.conv2(e)))\n",
    "        x2 = self.pool2(x2)\n",
    "        concat = torch.cat((x1, x2), dim=1).squeeze()\n",
    "        return torch.sigmoid(self.out(concat)).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4980a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(glove_vectors, split_names, epochs, batch_size, lr, k1,k2,n1,n2, freeze=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    datasets = {name: TextDataset(name) for name in split_names}\n",
    "    dataloaders = {name: torch.utils.data.DataLoader(\n",
    "        datasets[name], batch_size=batch_size, shuffle=False,\n",
    "        collate_fn=lambda batch: my_collate_function(batch, device)\n",
    "    ) for name in split_names}\n",
    "\n",
    "    model = CNNModel(glove_vectors, k1,k2,n1,n2, freeze=freeze).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    history = {\"train_loss\":[], \"val_loss\":[], \"test_loss\":[], \"accuracy\":[]}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X, y in dataloaders[\"train\"]:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        history[\"train_loss\"].append(train_loss/len(dataloaders[\"train\"]))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, test_loss, acc = 0,0,0\n",
    "        n_total = 0\n",
    "        with torch.no_grad():\n",
    "            # Validation\n",
    "            for Xv, yv in dataloaders[\"validation\"]:\n",
    "                logits_v = model(Xv)\n",
    "                val_loss += criterion(logits_v, yv).item()\n",
    "            # Test\n",
    "            for Xt, yt in dataloaders[\"test\"]:\n",
    "                logits_t = model(Xt)\n",
    "                test_loss += criterion(logits_t, yt).item()\n",
    "                preds = torch.round(torch.sigmoid(logits_t))\n",
    "                acc += (preds==yt).sum().item()\n",
    "                n_total += len(yt)\n",
    "        history[\"val_loss\"].append(val_loss/len(dataloaders[\"validation\"]))\n",
    "        history[\"test_loss\"].append(test_loss/len(dataloaders[\"test\"]))\n",
    "        history[\"accuracy\"].append(acc/n_total)\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57bc7bba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Config' from 'torch.utils._config_module' (c:\\Users\\goran\\anaconda3\\envs\\ece1786\\Lib\\site-packages\\torch\\utils\\_config_module.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m overfit_model, overfit_history \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[0;32m      2\u001b[0m     glove_vectors, split_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverfit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverfit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverfit\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      3\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, k1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,k2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,n1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,n2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(overfit_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverfit Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(overfit_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverfit Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(glove_vectors, split_names, epochs, batch_size, lr, k1, k2, n1, n2, freeze)\u001b[0m\n\u001b[0;32m      5\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m {name: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m      6\u001b[0m     datasets[name], batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m batch: my_collate_function(batch, device)\n\u001b[0;32m      8\u001b[0m ) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m split_names}\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m CNNModel(glove_vectors, k1,k2,n1,n2, freeze\u001b[38;5;241m=\u001b[39mfreeze)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     12\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m     14\u001b[0m history \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:[], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:[], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:[], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m:[]}\n",
      "File \u001b[1;32mc:\\Users\\goran\\anaconda3\\envs\\ece1786\\Lib\\site-packages\\torch\\optim\\adam.py:78\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m capturable \u001b[38;5;129;01mand\u001b[39;00m foreach:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     76\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas[0] as a Tensor is not supported for capturable=False and foreach=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m         )\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m betas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor betas[0] must be 1-element\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(betas[\u001b[38;5;241m1\u001b[39m], Tensor):\n",
      "File \u001b[1;32mc:\\Users\\goran\\anaconda3\\envs\\ece1786\\Lib\\site-packages\\torch\\optim\\optimizer.py:371\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\goran\\anaconda3\\envs\\ece1786\\Lib\\site-packages\\torch\\_compile.py:27\u001b[0m, in \u001b[0;36minner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;129m@overload\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_disable_dynamo\u001b[39m(\n\u001b[0;32m     23\u001b[0m     fn: Literal[\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, recursive: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     24\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable[[Callable[_P, _T]], Callable[_P, _T]]: \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_disable_dynamo\u001b[39m(\n\u001b[0;32m     28\u001b[0m     fn: Optional[Callable[_P, _T]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, recursive: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Callable[_P, _T], Callable[[Callable[_P, _T]], Callable[_P, _T]]]:\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    This API should be only used inside torch, external users should still use\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    torch._dynamo.disable. The main goal of this API is to avoid circular\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    the invocation of the decorated function.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\goran\\anaconda3\\envs\\ece1786\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, convert_frame, eval_frame, resume_execution\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[1;32mc:\\Users\\goran\\anaconda3\\envs\\ece1786\\Lib\\site-packages\\torch\\_dynamo\\config.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Literal, Optional, TYPE_CHECKING, Union\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_environment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fbcode\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config, get_tristate_env, install_config_module\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# to configure logging for dynamo, aot, and inductor\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# use the following API in the torch._logging module\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# torch._logging.set_logs(dynamo=<level>, aot=<level>, inductor<level>)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# the name of a file to write the logs to\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# [@compile_ignored: debug]\u001b[39;00m\n\u001b[0;32m     33\u001b[0m log_file_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Config' from 'torch.utils._config_module' (c:\\Users\\goran\\anaconda3\\envs\\ece1786\\Lib\\site-packages\\torch\\utils\\_config_module.py)"
     ]
    }
   ],
   "source": [
    "overfit_model, overfit_history = train_model(\n",
    "    glove_vectors, split_names=[\"overfit\",\"overfit\",\"overfit\"],\n",
    "    epochs=1500, batch_size=4, lr=0.001, k1=2,k2=4,n1=50,n2=50\n",
    ")\n",
    "\n",
    "plt.plot(overfit_history[\"train_loss\"], label=\"Overfit Loss\")\n",
    "plt.plot(overfit_history[\"accuracy\"], label=\"Overfit Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(overfit_history[\"accuracy\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d7107",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train_model(\n",
    "    glove_vectors, split_names=[\"train\",\"validation\",\"test\"],\n",
    "    epochs=50, batch_size=4, lr=0.001, k1=2,k2=4,n1=20,n2=20\n",
    ")\n",
    "\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.plot(history[\"test_loss\"], label=\"Test Loss\")\n",
    "plt.plot(history[\"accuracy\"], label=\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "torch.save(model.state_dict(), \"CNNModel.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, history_ft = train_model(\n",
    "    glove_vectors, split_names=[\"train\",\"validation\",\"test\"],\n",
    "    epochs=50, batch_size=4, lr=0.001, k1=2,k2=4,n1=20,n2=20, freeze=False\n",
    ")\n",
    "\n",
    "torch.save(model_ft.state_dict(), \"CNNModel_FT.pt\")\n",
    "print(history_ft[\"accuracy\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please create a folder called 'data' in colab and put 'overfit.tsv' 'train.tsv' 'test.tsv' 'validation.tsv' into 'data' folder\n",
    "embedding, totalLoss, totalValidationLoss, totalTestLoss, totalAccuray = main(50,4,0.001,2,4,20,20)\n",
    "plt.plot(totalLoss, label='Train Loss')\n",
    "plt.plot(totalValidationLoss, label='Validation Loss')\n",
    "plt.plot(totalTestLoss, label='Test Loss')\n",
    "plt.plot(totalAccuray, label='Accuray')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee4a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, history_ft = train_model(\n",
    "    glove_vectors, split_names=[\"train\",\"validation\",\"test\"],\n",
    "    epochs=50, batch_size=4, lr=0.001, k1=2,k2=4,n1=20,n2=20, freeze=False\n",
    ")\n",
    "\n",
    "torch.save(model_ft.state_dict(), \"CNNModel_FT.pt\")\n",
    "print(history_ft[\"accuracy\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(totalAccuray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde084d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModelNoFreeze(torch.nn.Module):\n",
    "  def __init__(self, vocab,k1,k2,n1,n2):\n",
    "    super().__init__()\n",
    "    self.k1 = (k1, 100)\n",
    "    self.k2 = (k2, 100)\n",
    "    self.n1 = n1\n",
    "    self.n2 = n2\n",
    "    self.probabilityFunction = torch.nn.Sigmoid()\n",
    "\n",
    "    self.embedding = torch.nn.Embedding.from_pretrained(vocab.vectors,freeze=False)\n",
    "\n",
    "    self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=self.n1, kernel_size=self.k1, bias=False)\n",
    "    self.bn1 = torch.nn.BatchNorm2d(self.n1)\n",
    "    self.maxpool1 = torch.nn.AdaptiveMaxPool2d(output_size=(1, 1))\n",
    "\n",
    "    self.conv2 = torch.nn.Conv2d(in_channels=1, out_channels=self.n2, kernel_size=self.k2, bias=False)\n",
    "    self.bn2 = torch.nn.BatchNorm2d(self.n2)\n",
    "    self.maxpool2 = torch.nn.AdaptiveMaxPool2d(output_size=(1, 1))\n",
    "\n",
    "    self.out = torch.nn.Linear(self.n1+self.n2, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    e = self.embedding(x)\n",
    "    input = torch.transpose(e, 0, 1).unsqueeze(1)\n",
    "    x1 = self.conv1(input)\n",
    "    x1 = F.relu(x1)\n",
    "    x1 = self.bn1(x1)\n",
    "    x1 = self.maxpool1(x1)\n",
    "\n",
    "    x2 = self.conv2(input)\n",
    "    x2 = F.relu(x2)\n",
    "    x2 = self.bn2(x2)\n",
    "    x2 = self.maxpool2(x2)\n",
    "\n",
    "    concatenate = torch.cat((x1, x2), dim=1)\n",
    "    output = self.out(concatenate.squeeze())\n",
    "    logits = self.probabilityFunction(output)\n",
    "\n",
    "    return logits.reshape([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainNoFreeze(epochs,batchSize,learningRate,k1,k2,n1,n2):\n",
    "    #   fix seed\n",
    "    torch.manual_seed(2)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print (\"Using device:\", device)\n",
    "\n",
    "    ### 3.3 Processing of the data ###\n",
    "\n",
    "\n",
    "    # 3.3.2\n",
    "\n",
    "    train_dataset = TextDataset(glove, \"train\")\n",
    "    val_dataset = TextDataset(glove, \"validation\")\n",
    "    test_dataset = TextDataset(glove, \"test\")\n",
    "\n",
    "    # 3.3.3\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batchSize,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda batch: my_collate_function(batch, device))\n",
    "    validation_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batchSize,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda batch: my_collate_function(batch, device))\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batchSize,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda batch: my_collate_function(batch, device))\n",
    "\n",
    "    # Instantiate your model(s) and train them and so on\n",
    "    # We suggest parameterizing the model - k1, n1, k2, n2, and other hyperparameters\n",
    "    # so that it is easier to experiment with\n",
    "\n",
    "    #4.3\n",
    "    model = CNNModelNoFreeze(glove,k1=k1,k2=k2,n1=n1,n2=n2)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learningRate)\n",
    "    lossFunction = torch.nn.BCEWithLogitsLoss()\n",
    "    probabilityFunction = torch.nn.Sigmoid()\n",
    "\n",
    "    totalLoss = []\n",
    "    totalTestLoss = []\n",
    "    totalValidationLoss = []\n",
    "    totalAccuray = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "      print(i)\n",
    "      currentEpochLoss = 0\n",
    "      currentEpochLossV = 0\n",
    "      currentEpochLossT = 0\n",
    "      currentEpochValidationLoss = 0\n",
    "      for X_train, y_train in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x=X_train)\n",
    "        currentLoss = lossFunction(logits, y_train.float())\n",
    "        currentEpochLoss = currentEpochLoss + currentLoss\n",
    "        currentLoss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "      averageCurrentEpochLoss = currentEpochLoss/len(train_dataloader)\n",
    "      totalLoss.append(averageCurrentEpochLoss.item())\n",
    "\n",
    "      for X_validation, y_validation in validation_dataloader:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "          logitsV = model(x=X_validation)\n",
    "        currentLossV = lossFunction(logitsV, y_validation.float())\n",
    "        currentEpochLossV = currentEpochLossV + currentLossV\n",
    "\n",
    "      averageCurrentEpochLossV = currentEpochLossV/len(validation_dataloader)\n",
    "      totalValidationLoss.append(averageCurrentEpochLossV.item())\n",
    "\n",
    "    #4.5\n",
    "      currentAccuray = 0\n",
    "      numbers = 0\n",
    "      for X_test, y_test in test_dataloader:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "          logitsT = model(x=X_test)\n",
    "        currentLossT = lossFunction(logitsT, y_test.float())\n",
    "        currentEpochLossT = currentEpochLossT + currentLossT\n",
    "\n",
    "        probability = probabilityFunction(logitsT)\n",
    "        probability = torch.maximum(probability, torch.tensor([1e-5]))\n",
    "        probability = torch.minimum(probability, torch.tensor([0.99999]))\n",
    "        Y_pred = torch.round(probability)\n",
    "\n",
    "        n = 0\n",
    "        for eachy_test in y_test:\n",
    "          if eachy_test.item() == Y_pred[n]:\n",
    "            currentAccuray  = currentAccuray + 1\n",
    "          n = n + 1\n",
    "        numbers = numbers + len(Y_pred)\n",
    "\n",
    "      totalEpochAccuray = currentAccuray/numbers\n",
    "      totalAccuray.append(totalEpochAccuray)\n",
    "\n",
    "      averageCurrentEpochLossT = currentEpochLossT/len(validation_dataloader)\n",
    "      totalTestLoss.append(averageCurrentEpochLossT.item())\n",
    "\n",
    "    #4.7\n",
    "    torch.save(model.state_dict(), '/content/CNNNoFreezeModel')\n",
    "\n",
    "\n",
    "    return model, totalLoss, totalValidationLoss, totalTestLoss, totalAccuray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41eb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingNoFreeze, totalLossNoFreeze, totalValidationLossNoFreeze, totalTestLossNoFreeze, totalAccurayNoFreeze = mainNoFreeze(50,4,0.001,2,4,20,20) #n1,2-20 0.903\n",
    "plt.plot(totalLossNoFreeze, label='Train Loss')\n",
    "plt.plot(totalValidationLossNoFreeze, label='Validation Loss')\n",
    "plt.plot(totalTestLossNoFreeze, label='Test Loss')\n",
    "plt.plot(totalAccurayNoFreeze, label='Accuray')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
